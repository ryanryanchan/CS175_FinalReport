{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ryan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import listdir\n",
    "from scipy.misc import imread, imresize\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4124\n"
     ]
    }
   ],
   "source": [
    "## ================== preprocessing ======================== ##\n",
    "\n",
    "# Settings:\n",
    "img_size = 64\n",
    "grayscale_images = True\n",
    "num_class = 10\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "def get_img(data_path):\n",
    "    # Getting image array from path:\n",
    "    img = imread(data_path, flatten=grayscale_images)\n",
    "    img = imresize(img, (img_size, img_size, 1 if grayscale_images else 3))\n",
    "    return img\n",
    "\n",
    "def get_dataset(dataset_path='Dataset'):\n",
    "    # Getting all data from data path:\n",
    "    try:\n",
    "        X = np.load('npy_dataset/X.npy')\n",
    "        Y = np.load('npy_dataset/Y.npy')\n",
    "        \n",
    "        img = np.load('npy_dataset/X.npy')\n",
    "        lbl = np.load('npy_dataset/Y.npy')\n",
    "\n",
    "        data = []\n",
    "        label = []\n",
    "        for i in zip(img,lbl):\n",
    "            data.append(i[0])\n",
    "            data.append(np.flip(i[0],1))\n",
    "            label.append(i[1])\n",
    "            label.append(i[1])\n",
    "            \n",
    "    except:\n",
    "        labels = listdir(dataset_path) # Geting labels\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i, label in enumerate(labels):\n",
    "            datas_path = dataset_path+'/'+label\n",
    "            for data in listdir(datas_path):\n",
    "                img = get_img(datas_path+'/'+data)\n",
    "                X.append(img)\n",
    "                Y.append(i)\n",
    "        # Create dateset:\n",
    "        X = 1-np.array(X).astype('float32')/255.\n",
    "        Y = np.array(Y).astype('float32')\n",
    "        Y = to_categorical(Y, num_class)\n",
    "        if not os.path.exists('npy_dataset/'):\n",
    "            os.makedirs('npy_dataset/')\n",
    "        np.save('npy_dataset/X.npy', X)\n",
    "        np.save('npy_dataset/Y.npy', Y)\n",
    "        \n",
    "        img = np.load('npy_dataset/X.npy')\n",
    "        lbl = np.load('npy_dataset/Y.npy')\n",
    "\n",
    "        data = []\n",
    "        label = []\n",
    "        for i in zip(img,lbl):\n",
    "            data.append(i[0])\n",
    "            data.append(np.flip(i[0],1))\n",
    "            label.append(i[1])\n",
    "            label.append(i[1])\n",
    "        # plt.imshow(data[500])\n",
    "        # print(label[500])\n",
    "        np.save('npy_dataset/X.npy', data)\n",
    "        np.save('npy_dataset/Y.npy', label)\n",
    "        \n",
    "        \n",
    "    return X, Y\n",
    "\n",
    "X, Y = get_dataset()\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COVNET\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 64,64], name='X')\n",
    "x_image = tf.reshape(x, [-1, 64, 64, 1])\n",
    "# Placeholder variable for the true labels associated with the images\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input, num_input_channels, filter_size, num_filters, name,stride=[1, 1, 1, 1]):\n",
    "    \n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # Shape of the filter-weights for the convolution\n",
    "        shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "        # Create new weights (filters) with the given shape\n",
    "        weights = tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "        # Create new biases, one for each filter\n",
    "        biases = tf.Variable(tf.constant(0.05, shape=[num_filters]))\n",
    "\n",
    "        # TensorFlow operation for convolution\n",
    "        layer = tf.nn.conv2d(input=input, filter=weights, strides=stride, padding='SAME')\n",
    "\n",
    "        # Add the biases to the results of the convolution.\n",
    "        layer += biases\n",
    "        \n",
    "        return layer, weights\n",
    "    \n",
    "def new_pool_layer(input,name,stride=[1,2,2,1],ksize=[1,2,2,1]):\n",
    "  with tf.variable_scope(name) as scope:\n",
    "    layer = tf.nn.max_pool(input,strides=[1,2,2,1],ksize=[1,2,2,1],padding='SAME')\n",
    "    return layer\n",
    "\n",
    "def new_relu_layer(input,name):\n",
    "  with tf.variable_scope(name) as scope:\n",
    "        layer = tf.nn.relu(input)\n",
    "        return layer\n",
    "    \n",
    "\n",
    "def new_fc_layer(input, num_inputs, num_outputs, name):\n",
    "  with tf.variable_scope(name) as scope:\n",
    "      # Create new weights and biases.\n",
    "      weights = tf.Variable(tf.truncated_normal([num_inputs, num_outputs], stddev=0.05))\n",
    "      biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))\n",
    "      # Multiply the input and weights, and then add the bias-values.\n",
    "      layer = tf.matmul(input, weights) + biases\n",
    "      return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conv1, weights_conv1 = new_conv_layer(input=x_image, num_input_channels=1, filter_size=3, num_filters=6, name =\"conv1\")\n",
    "layer_pool1 = new_pool_layer(layer_conv1,'pool1')\n",
    "layer_relu1  =new_relu_layer(layer_pool1,'relu1')\n",
    "\n",
    "layer_conv2, weights_conv2 = new_conv_layer(input=layer_relu1, num_input_channels=6, filter_size=3, num_filters=32, name =\"conv2\")\n",
    "layer_pool2 = new_pool_layer(layer_conv2,'pool2')\n",
    "layer_relu2  =new_relu_layer(layer_pool2,'relu2')\n",
    "\n",
    "layer_conv3, weights_conv3 = new_conv_layer(input=layer_relu2, num_input_channels=32, filter_size=3, num_filters=64, name =\"conv3\")\n",
    "layer_pool3 = new_pool_layer(layer_conv3,'pool3')\n",
    "layer_relu3  =new_relu_layer(layer_pool3,'relu3')\n",
    "\n",
    "num_features = layer_relu3.get_shape()[1:4].num_elements()\n",
    "layer_flat = tf.reshape(layer_relu3, [-1, num_features])\n",
    "layer_fc1 = new_fc_layer(layer_flat, num_inputs=num_features, num_outputs=128,name='fc1')\n",
    "\n",
    "### Batch Normalisation\n",
    "layer_bn = tf.contrib.layers.batch_norm(layer_fc1 ,center=True, scale=True)\n",
    "layer_relu4 = new_relu_layer(layer_bn, name=\"relu3\")\n",
    "layer_fc2 = new_fc_layer(input=layer_relu4, num_inputs=128, num_outputs=10, name=\"fc2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "  y_pred = tf.nn.softmax(layer_fc2)\n",
    "  y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "    \n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=layer_fc2, labels=y_true)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('opt'):\n",
    "  optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 \tTraining Loss: 1.007 \tValidation Loss: 0.994 \tAccuracy: 77.21%\n",
      "Epoch: 2/20 \tTraining Loss: 0.645 \tValidation Loss: 0.616 \tAccuracy: 85.21%\n",
      "Epoch: 3/20 \tTraining Loss: 0.458 \tValidation Loss: 0.455 \tAccuracy: 88.73%\n",
      "Epoch: 4/20 \tTraining Loss: 0.364 \tValidation Loss: 0.351 \tAccuracy: 92.61%\n",
      "Epoch: 5/20 \tTraining Loss: 0.303 \tValidation Loss: 0.270 \tAccuracy: 94.67%\n",
      "Epoch: 6/20 \tTraining Loss: 0.214 \tValidation Loss: 0.198 \tAccuracy: 97.58%\n",
      "Epoch: 7/20 \tTraining Loss: 0.139 \tValidation Loss: 0.157 \tAccuracy: 97.82%\n",
      "Epoch: 8/20 \tTraining Loss: 0.083 \tValidation Loss: 0.121 \tAccuracy: 98.67%\n",
      "Epoch: 9/20 \tTraining Loss: 0.100 \tValidation Loss: 0.086 \tAccuracy: 99.15%\n",
      "Epoch: 10/20 \tTraining Loss: 0.078 \tValidation Loss: 0.066 \tAccuracy: 99.52%\n",
      "Epoch: 11/20 \tTraining Loss: 0.059 \tValidation Loss: 0.045 \tAccuracy: 99.88%\n",
      "Epoch: 12/20 \tTraining Loss: 0.040 \tValidation Loss: 0.037 \tAccuracy: 99.76%\n",
      "Epoch: 13/20 \tTraining Loss: 0.033 \tValidation Loss: 0.032 \tAccuracy: 100.00%\n",
      "Epoch: 14/20 \tTraining Loss: 0.053 \tValidation Loss: 0.022 \tAccuracy: 99.88%\n",
      "Epoch: 15/20 \tTraining Loss: 0.020 \tValidation Loss: 0.019 \tAccuracy: 100.00%\n",
      "Epoch: 16/20 \tTraining Loss: 0.018 \tValidation Loss: 0.016 \tAccuracy: 100.00%\n",
      "Epoch: 17/20 \tTraining Loss: 0.013 \tValidation Loss: 0.014 \tAccuracy: 100.00%\n",
      "Epoch: 18/20 \tTraining Loss: 0.013 \tValidation Loss: 0.010 \tAccuracy: 99.88%\n",
      "Epoch: 19/20 \tTraining Loss: 0.013 \tValidation Loss: 0.009 \tAccuracy: 100.00%\n",
      "Epoch: 20/20 \tTraining Loss: 0.007 \tValidation Loss: 0.007 \tAccuracy: 100.00%\n",
      "running time: 0:00:07.524884\n"
     ]
    }
   ],
   "source": [
    "def printResult(epoch, numberOfEpoch, trainLoss, validationLoss, validationAccuracy):\n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, numberOfEpoch),\n",
    "         '\\tTraining Loss: {:.3f}'.format(trainLoss),\n",
    "         '\\tValidation Loss: {:.3f}'.format(validationLoss),\n",
    "         '\\tAccuracy: {:.2f}%'.format(validationAccuracy*100))\n",
    "\n",
    "num_epochs = 20\n",
    "batchSize = 100\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "start = dt.now()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    for epoch in range(num_epochs):\n",
    "        # training data & validation data\n",
    "        train_x, val_x, train_y, val_y = train_test_split(X, Y,\\\n",
    "                                                      test_size = 0.2)   \n",
    "        # training loss\n",
    "        for i in range(0, len(train_x), 100):\n",
    "            trainLoss, _= sess.run([cost, optimizer], feed_dict = {\n",
    "                x: train_x[i: i+batchSize],\n",
    "                y_true: train_y[i: i+batchSize]\n",
    "            })\n",
    "            \n",
    "        # validation loss\n",
    "        valAcc, valLoss = sess.run([accuracy, cost], feed_dict ={\n",
    "            x: val_x,\n",
    "            y_true: val_y,})\n",
    "        \n",
    "        \n",
    "        # print out\n",
    "        printResult(epoch, num_epochs, trainLoss, valLoss, valAcc)\n",
    "        \n",
    "print('running time:', dt.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ======================== SqueezeNet =========================== ##\n",
    "\n",
    "# define squeeze module\n",
    "def squeeze(input, channels, layer_num):\n",
    "    \"\"\"\n",
    "    Defines squeezed block for fire module.\n",
    "\n",
    "    :param input: input tensor\n",
    "    :param channels: number of output channels\n",
    "    :param layer_num: layer number for naming purposes\n",
    "    :return: output tensor convoluted with squeeze layer\n",
    "    \"\"\"\n",
    "    layer_name = 'squeeze_' + str(layer_num)\n",
    "    input_channels = input.get_shape().as_list()[3]\n",
    "\n",
    "    with tf.name_scope(layer_name):\n",
    "        weights = tf.Variable(tf.contrib.layers.xavier_initializer()([1, 1, input_channels, channels]))\n",
    "        biases = tf.Variable(tf.zeros([1, 1, 1, channels]), name='biases')\n",
    "        onebyone = tf.nn.conv2d(input, weights, strides=(1, 1, 1, 1), padding='VALID') + biases\n",
    "        A = tf.nn.relu(onebyone)\n",
    "\n",
    "    return A\n",
    "\n",
    "# define expand module\n",
    "def expand(input, channels_1by1, channels_3by3, layer_num):\n",
    "    \"\"\"\n",
    "    Defines expand block for fire module.\n",
    "    :param input: input tensor\n",
    "    :param channels_1by1: number of output channels in 1x1 layers\n",
    "    :param channels_3by3: number of output channels in 3x3 layers\n",
    "    :param layer_num: layer number for naming purposes\n",
    "    :return: output tensor convoluted with expand layer\n",
    "    \"\"\"\n",
    "\n",
    "    layer_name = 'expand_' + str(layer_num)\n",
    "    input_channels = input.get_shape().as_list()[3]\n",
    "\n",
    "    with tf.name_scope(layer_name):\n",
    "        weights1x1 = tf.Variable(tf.contrib.layers.xavier_initializer()([1, 1, input_channels, channels_1by1]))\n",
    "        biases1x1 = tf.Variable(tf.zeros([1, 1, 1, channels_1by1]), name='biases')\n",
    "        onebyone = tf.nn.conv2d(input, weights1x1, strides=(1, 1, 1, 1), padding='VALID') + biases1x1\n",
    "        A_1x1 = tf.nn.relu(onebyone)\n",
    "\n",
    "        weights3x3 = tf.Variable(tf.contrib.layers.xavier_initializer()([1, 1, input_channels, channels_3by3]))\n",
    "        biases3x3 = tf.Variable(tf.zeros([1, 1, 1, channels_3by3]), name='biases')\n",
    "        threebythree = tf.nn.conv2d(input, weights3x3, strides=(1, 1, 1, 1), padding='SAME') + biases3x3\n",
    "        A_3x3 = tf.nn.relu(threebythree)\n",
    "\n",
    "    return tf.concat([A_1x1, A_3x3], axis=3)\n",
    "\n",
    "\n",
    "# define fire module\n",
    "def fire_module(input, squeeze_channels, expand_channels_1by1, expand_channels_3by3, layer_num):\n",
    "    \"\"\"\n",
    "    Train fire module. Fire module does not change input height and width, only depth.\n",
    "    :param input: input tensor\n",
    "    :param squeeze_channels: number of channels for 1x1 squeeze layer\n",
    "    :param expand_channels_1by1: number of channels for 1x1 expand layer\n",
    "    :param expand_channels_3by3: number of channels for 3x3 expand layer\n",
    "    :param layer_num: number of layer for naming purposes only\n",
    "    :return: a tensor of shape [input_height x input_width x expand_channels_1by1 * expand_channels_3by3]\n",
    "    \"\"\"\n",
    "    with tf.name_scope('fire_' + str(layer_num)):\n",
    "        squeeze_output = squeeze(input, squeeze_channels, layer_num)\n",
    "        return expand(squeeze_output, expand_channels_1by1, expand_channels_3by3, layer_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = input_width = 64\n",
    "input_channels = 1\n",
    "output_classes = 10\n",
    "pooling_size=(1, 3, 3, 1)\n",
    "\n",
    "input_image = tf.placeholder(tf.float32,shape=[None, input_height, input_width, 1],name='input_image')\n",
    "labels = tf.placeholder(tf.float32, shape=[None,10])\n",
    "labels_cls = tf.argmax(labels, axis=1)\n",
    "in_training = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "# layer 1 - conv 1\n",
    "W_conv1 = tf.Variable(tf.contrib.layers.xavier_initializer()([7, 7, 1, 96]))\n",
    "b_conv1 = tf.Variable(tf.zeros([1, 1, 1, 96]))\n",
    "X_1 = tf.nn.conv2d(input_image, W_conv1, strides=(1, 2, 2, 1), padding='VALID') + b_conv1\n",
    "A_1 = tf.nn.relu(X_1)\n",
    "\n",
    "# layer 2 - maxpool\n",
    "maxpool_1 = tf.nn.max_pool(A_1, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_1')\n",
    "\n",
    "# layer 3-5 - fire modules\n",
    "fire_2 = fire_module(maxpool_1, 16, 64, 64, layer_num=2)\n",
    "fire_3 = fire_module(fire_2, 16, 64, 64, layer_num=3)\n",
    "fire_4 = fire_module(fire_3, 32, 128, 128, layer_num=4)\n",
    "\n",
    "# layer 6 - maxpool\n",
    "maxpool_4 = tf.nn.max_pool(fire_4, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_4')\n",
    "\n",
    "# layer 7-10 - fire modules\n",
    "fire_5 = fire_module(maxpool_4, 32, 128, 128, layer_num=5)\n",
    "fire_6 = fire_module(fire_5, 48, 192, 192, layer_num=6)\n",
    "fire_7 = fire_module(fire_6, 48, 192, 192, layer_num=7)\n",
    "fire_8 = fire_module(fire_7, 64, 256, 256, layer_num=8)\n",
    "\n",
    "# layer 11 - maxpool\n",
    "maxpool_8 = tf.nn.max_pool(fire_8, ksize=pooling_size, strides=(1, 2, 2, 1), padding='VALID', name='maxpool_8')\n",
    "\n",
    "# layer 12 - fire 9 + dropout\n",
    "fire_9 = fire_module(maxpool_8, 64, 256, 256, layer_num=9)\n",
    "dropout_9 = tf.cond(in_training,lambda: tf.nn.dropout(fire_9, keep_prob=0.5),lambda: fire_9)\n",
    "\n",
    "# layer 13 - final\n",
    "W_conv10 = tf.Variable(tf.contrib.layers.xavier_initializer()([1, 1, 512, output_classes]))\n",
    "b_conv10 = tf.Variable(tf.zeros([1, 1, 1, output_classes]))\n",
    "conv_10 = tf.nn.conv2d(dropout_9, W_conv10, strides=(1, 1, 1,1), padding='VALID') + b_conv10            \n",
    "A_conv_10 = tf.nn.relu(conv_10)\n",
    "\n",
    "# avg pooling to get [1 x 1 x num_classes] must average over entire window oh H x W from input layer\n",
    "_, H_last, W_last, _ = A_conv_10.get_shape().as_list()\n",
    "pooled = tf.nn.avg_pool(A_conv_10, ksize=(1, H_last, W_last, 1), strides=(1, 1, 1, 1), padding='VALID')\n",
    "logits = tf.squeeze(pooled, axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-9fee7e00298e>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "# loss + optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(0.0001).minimize(cost)\n",
    "\n",
    "\n",
    "labels_cls = tf.cast(labels_cls,tf.float32)\n",
    "y_pred_cls = tf.cast(y_pred_cls,tf.float32)\n",
    "# accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_cls, labels_cls), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ryan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "c:\\users\\ryan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/300 \tTraining Loss: 2.303 \tValidation Loss: 2.303 \tAccuracy: 10.18%\n",
      "Epoch: 10/300 \tTraining Loss: 1.853 \tValidation Loss: 1.859 \tAccuracy: 23.76%\n",
      "Epoch: 20/300 \tTraining Loss: 1.589 \tValidation Loss: 1.581 \tAccuracy: 38.91%\n",
      "Epoch: 30/300 \tTraining Loss: 1.487 \tValidation Loss: 1.252 \tAccuracy: 50.79%\n",
      "Epoch: 40/300 \tTraining Loss: 1.075 \tValidation Loss: 1.158 \tAccuracy: 53.45%\n",
      "Epoch: 50/300 \tTraining Loss: 0.916 \tValidation Loss: 1.001 \tAccuracy: 59.39%\n",
      "Epoch: 60/300 \tTraining Loss: 0.928 \tValidation Loss: 1.067 \tAccuracy: 57.94%\n",
      "Epoch: 70/300 \tTraining Loss: 0.948 \tValidation Loss: 0.864 \tAccuracy: 65.45%\n",
      "Epoch: 80/300 \tTraining Loss: 0.692 \tValidation Loss: 0.801 \tAccuracy: 69.21%\n",
      "Epoch: 90/300 \tTraining Loss: 0.720 \tValidation Loss: 0.676 \tAccuracy: 72.61%\n",
      "Epoch: 100/300 \tTraining Loss: 0.692 \tValidation Loss: 0.826 \tAccuracy: 68.36%\n",
      "Epoch: 110/300 \tTraining Loss: 0.478 \tValidation Loss: 0.649 \tAccuracy: 75.52%\n",
      "Epoch: 120/300 \tTraining Loss: 0.511 \tValidation Loss: 0.592 \tAccuracy: 78.06%\n",
      "Epoch: 130/300 \tTraining Loss: 0.514 \tValidation Loss: 0.514 \tAccuracy: 81.09%\n",
      "Epoch: 140/300 \tTraining Loss: 0.436 \tValidation Loss: 0.462 \tAccuracy: 81.94%\n",
      "Epoch: 150/300 \tTraining Loss: 0.501 \tValidation Loss: 0.459 \tAccuracy: 82.42%\n",
      "Epoch: 160/300 \tTraining Loss: 0.443 \tValidation Loss: 0.481 \tAccuracy: 81.45%\n",
      "Epoch: 170/300 \tTraining Loss: 0.499 \tValidation Loss: 0.524 \tAccuracy: 79.76%\n",
      "Epoch: 180/300 \tTraining Loss: 0.314 \tValidation Loss: 0.489 \tAccuracy: 82.79%\n",
      "Epoch: 190/300 \tTraining Loss: 0.267 \tValidation Loss: 0.349 \tAccuracy: 87.64%\n",
      "Epoch: 200/300 \tTraining Loss: 0.298 \tValidation Loss: 0.360 \tAccuracy: 88.48%\n",
      "Epoch: 210/300 \tTraining Loss: 0.247 \tValidation Loss: 0.308 \tAccuracy: 88.97%\n",
      "Epoch: 220/300 \tTraining Loss: 0.197 \tValidation Loss: 0.280 \tAccuracy: 91.15%\n",
      "Epoch: 230/300 \tTraining Loss: 0.128 \tValidation Loss: 0.349 \tAccuracy: 88.61%\n",
      "Epoch: 240/300 \tTraining Loss: 0.251 \tValidation Loss: 0.240 \tAccuracy: 92.61%\n",
      "Epoch: 250/300 \tTraining Loss: 0.207 \tValidation Loss: 0.218 \tAccuracy: 93.21%\n",
      "Epoch: 260/300 \tTraining Loss: 0.182 \tValidation Loss: 0.226 \tAccuracy: 92.12%\n",
      "Epoch: 270/300 \tTraining Loss: 0.213 \tValidation Loss: 0.171 \tAccuracy: 95.03%\n",
      "Epoch: 280/300 \tTraining Loss: 0.194 \tValidation Loss: 0.220 \tAccuracy: 92.12%\n",
      "Epoch: 290/300 \tTraining Loss: 0.137 \tValidation Loss: 0.199 \tAccuracy: 92.36%\n",
      "Epoch: 300/300 \tTraining Loss: 0.231 \tValidation Loss: 0.214 \tAccuracy: 93.21%\n",
      "running time: 0:18:05.768809\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "num_epochs = 300\n",
    "batchSize = 500\n",
    "import time\n",
    "\n",
    "def printResult(epoch, numberOfEpoch, trainLoss, validationLoss, validationAccuracy):\n",
    "    print(\"Epoch: {}/{}\".format(epoch, numberOfEpoch),\n",
    "         '\\tTraining Loss: {:.3f}'.format(trainLoss),\n",
    "         '\\tValidation Loss: {:.3f}'.format(validationLoss),\n",
    "         '\\tAccuracy: {:.2f}%'.format(validationAccuracy*100))\n",
    "\n",
    "start = dt.now()\n",
    "    \n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs+1):\n",
    "        # training data & validation data\n",
    "        train_x, val_x, train_y, val_y = train_test_split(X, Y ,test_size = 0.2)  \n",
    "        # training loss\n",
    "        for i in range(0, len(train_x), 100):\n",
    "            trainLoss, _= sess.run([cost, optimizer], feed_dict = {\n",
    "                input_image: np.expand_dims(train_x[i: i+batchSize], axis=4),\n",
    "                labels: train_y[i: i+batchSize],\n",
    "                in_training: True\n",
    "            })\n",
    "            \n",
    "        # validation loss\n",
    "        valAcc, valLoss = sess.run([accuracy, cost], feed_dict ={\n",
    "            input_image: np.expand_dims(val_x, axis=4),\n",
    "            labels: val_y,\n",
    "            in_training: True})\n",
    "        \n",
    "        # print out\n",
    "        if(epoch%10 == 0):\n",
    "            printResult(epoch, num_epochs, trainLoss, valLoss, valAcc)\n",
    "            \n",
    "print('running time:', dt.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
